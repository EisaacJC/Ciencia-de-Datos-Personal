{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EisaacJC/Ciencia-de-Datos-Personal/blob/master/Comparison_between_discretization_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB1aIlHnd9qV"
      },
      "source": [
        " <font size=\"5\"><h3 align=\"center\">Comparativa de desempeño de algoritmos de discretización en diferentes algortimos de clasificación</h3></font>\n",
        "\n",
        "<div style=\"text-align: justify\">Resumen: Se implementó el algoritmo de discrestización conocido como class-attribute interdependence maximization (CAIM) [1]. Con la finalidad de obtener una comparativa, se realizaron discretizaciones usando una transformación por k-medias y cuantil, disponibles en la librería de sklearn.\n",
        "A partir de los conjuntos discretizados y adicionando los conjuntos originales se utilizaron tres algoritmos de clasificación: Naive Bayes, DecisionTreeClassifier y Support vector machine (SVM). </div>\n",
        "\n",
        "<div style=\"text-align: justify\">Los resultados de la clasificación muestran una mejora significativa al discretizar las bases de datos.\n",
        "</div>\n",
        "\n",
        "<font size=\"2.5\">Integrantes:</font> \\\\\n",
        "<font size=\"2.5\">Diana Itzel Vazquez Santiago </font> \\\\\n",
        "<font size=\"2.5\">Emmanuel Isaac Juarez Caballero </font> \\\\\n",
        "<font size=\"2.5\">Jesús Eduardo Hermosilla Díaz </font> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eObEIITq4Rco"
      },
      "source": [
        "## Methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyW4FkPU_3jr"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0wz5G-iAbeSN"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import seaborn as sns\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import svm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0VqSKlo1uQT"
      },
      "source": [
        "### Webreader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNhs5rcf4YoS"
      },
      "source": [
        "Para el proceso de pre-procesamiento se extrajeron 10 bases de datos provenientes del [Machine Learning Repository](\"https://archive.ics.uci.edu/ml/index.php\"), los cuales se encuentran un repositorio de [github](\"https://raw.githubusercontent.com/EisaacJC/datasets/main/\").\n",
        "\n",
        "Su lectura se realiza a partir de la función webreader y downloader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IcI4aclOH5oF"
      },
      "outputs": [],
      "source": [
        "#lectura de datos\n",
        "def webreader(url):\n",
        "    urls=pd.read_csv(url+\"enlaces.txt\", header=None)\n",
        "    urls=urls.values.tolist()\n",
        "    urlsp=[]\n",
        "    urlsp2=[]\n",
        "    for i in range(len(urls)):\n",
        "        urlsp.append(urls[i][0])\n",
        "        urlsp2.append(url+urls[i][0])\n",
        "    urls=urlsp\n",
        "    return urls,urlsp2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8kSXEfuGL7Sb"
      },
      "outputs": [],
      "source": [
        "urls=webreader(\"https://raw.githubusercontent.com/EisaacJC/datasets/main/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WOOGUY8ONkoO"
      },
      "outputs": [],
      "source": [
        "def downloader(urls):\n",
        "    urls=urls\n",
        "    for i in range(len(urls[0])):\n",
        "        url=urls[1][i]\n",
        "        urllib.request.urlretrieve(url, '/content/'+urls[0][i])\n",
        "    return True    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaE8QAhxSEKh",
        "outputId": "bc10abd4-7318-4b48-f04f-97a91bbf44ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "downloader(urls)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grCxI0H4064D"
      },
      "source": [
        "### CAIM code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJTi78N1AkF0"
      },
      "source": [
        "Para realizar la discretización CAIM se empleó el siguiente pseudo-código: \\\\\n",
        "\n",
        "Dado un conjunto de $M$ ejemplos, $S$ clases y $F_i$ atributos continuos. \\\\\n",
        "Para cada $F_i$ hacer: \\\\\n",
        "- Paso 1. \n",
        " - Encontrar el valor máximo ($d_n$), y mínimo ($d_0$) de $F_i$.\n",
        " - Formar un conjunto con todos los valores distintos de $F_i$ en orden ascendente, e inicializar todos los posibles intervalos límites $B$ con mínimo, máximo y todos los puntos medios de todos los pares adyacente en el conjunto.\n",
        " - Establecer el esquema de inicialización inicial como $D:\\{[d_0,d_n]\\}$, fijar $GlobalCAIM=0$. \n",
        "- Paso 2.\n",
        " - Inicializar $k=1$.\n",
        " - Agregar tentativamente los límites inferiores de $B$ que no estén en $D$ y calcular el valor $CAIM$ correspondiente.\n",
        " - Después de que todas las adiciones tentativas hayan sido probadas aceptar la que tiene el mayor valor de $CAIM$.\n",
        " - Si ($CAIM$ > $GlobalCAIM$ or $k< S$) actualizar $D$ con el límite aceptado en el paso anterior y fijar $GlobalCAIM=CAIM$, sino terminar.\n",
        " - Fijar $k=k+1$ y regresar al segundo punto del paso dos.\n",
        "\n",
        " Salida: El esquema $D$ discretizado.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JcGlRAOmuOGM"
      },
      "outputs": [],
      "source": [
        "def get_caim(scheme, data, target):\n",
        "  B = []\n",
        "  for p in scheme[1:-1]:\n",
        "    B.append(np.where(data > p)[0][0])\n",
        "\n",
        "  B.insert(0, 0)\n",
        "  B.append(data.shape[0])\n",
        "  n = len(B) - 1\n",
        "  isum = 0\n",
        "  for j in range(n):\n",
        "    init = B[j]\n",
        "    fin = B[j + 1]\n",
        "    Mr = data[init:fin].shape[0]\n",
        "    val, counts = np.unique(target[init:fin], return_counts=True)\n",
        "    maxr = counts.max()\n",
        "    isum = isum + (maxr / Mr) * maxr\n",
        "  return isum/n\n",
        "\n",
        "def transform(X, scheme):\n",
        "  X_di = X.copy()\n",
        "  for j in range(X.shape[1]):\n",
        "    sh = scheme[j]\n",
        "    sh[-1] = sh[-1] + 1\n",
        "    xj = X[:, j]\n",
        "    for i in range(len(sh) - 1):\n",
        "      ind = np.where((xj >= sh[i]) & (xj < sh[i + 1]))[0]\n",
        "      X_di[ind, j] = i\n",
        "  return X_di\n",
        "\n",
        "def fit(X, target):\n",
        "  split_scheme = dict()\n",
        "  min_splits = np.unique(target).shape[0]\n",
        "\n",
        "  for j in range(X.shape[1]):\n",
        "    xj = X[:, j]\n",
        "    xj = xj[np.invert(np.isnan(xj))]\n",
        "    new_index = xj.argsort()\n",
        "    xj = xj[new_index]\n",
        "    yj = target[new_index]\n",
        "    allsplits = np.unique(xj)[1:-1].tolist()\n",
        "    global_caim = -1\n",
        "    mainscheme = [xj[0], xj[-1]]\n",
        "    best_caim = 0\n",
        "    k = 1\n",
        "\n",
        "    while (k <= min_splits) or ((global_caim < best_caim) and (allsplits)):\n",
        "      split_points = np.random.permutation(allsplits).tolist()\n",
        "      best_scheme = None\n",
        "      best_point = None\n",
        "      best_caim = 0\n",
        "      k = k + 1\n",
        "      while split_points:\n",
        "        scheme = mainscheme[:]\n",
        "        B = split_points.pop()\n",
        "        scheme.append(B)\n",
        "        scheme.sort()\n",
        "        c = get_caim(scheme, xj, yj)\n",
        "        if c > best_caim:\n",
        "          best_caim = c\n",
        "          best_scheme = scheme\n",
        "          best_point = B\n",
        "      if (k <= min_splits) or (best_caim > global_caim):\n",
        "        mainscheme = best_scheme\n",
        "        global_caim = best_caim\n",
        "        allsplits.remove(best_point)\n",
        "    split_scheme[j] = mainscheme\n",
        "  return split_scheme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MFeeNj2r4vv2"
      },
      "outputs": [],
      "source": [
        "def main_caim(x,y):\n",
        "  return transform(x,fit(x,y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoIQOqdT_A4Z"
      },
      "source": [
        "### Naive Bayes classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC-fbe_vJMJ0"
      },
      "source": [
        "<div style=\"text-align: justify\">Uno de los clasificadores más conocidos, simples y eficientes es el clasificador Naive Bayes el cual es un modelo probabilístico de aprendizaje automático que se utiliza para tareas de clasificación de datos, su funcionamiento se basa en la probabilidad condicional y el teorema de Bayes.</div>\n",
        "\n",
        "\\begin{align}\n",
        "P(C|F_1,...,F_n) = \\frac{P(C)P(F_1,...,F_n|C)}{P(F_1,...,F_n)}\n",
        "\\end{align}\n",
        "\n",
        "Se utilizó la implementación desarrollada para la tarea uno de la materia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P6jm30Gg_E-B"
      },
      "outputs": [],
      "source": [
        "def training(data_train, target_train, trainingSize, numFeatures):\n",
        "    casesByFeature = {}\n",
        "    data = []\n",
        "    for i in range(trainingSize):\n",
        "        if not target_train[i] in casesByFeature.keys():\n",
        "            casesByFeature[target_train[i]] = 1\n",
        "        else:\n",
        "            casesByFeature[target_train[i]] += 1\n",
        "        data.append([data_train[i], target_train[i]])\n",
        "    return casesByFeature, data\n",
        "\n",
        "def classify(numFeatures, trainingSize, casesByFeature, data, test):\n",
        "    solve = None\n",
        "    max_arg = 0\n",
        "    for y in casesByFeature.keys():\n",
        "        prob = casesByFeature[y]/trainingSize\n",
        "        for i in range(numFeatures):\n",
        "            cases = [x for x in data if x[0][i] == test[i] and x[1] == y]\n",
        "            n = len(cases)\n",
        "            prob *= n/trainingSize\n",
        "        if prob > max_arg:\n",
        "            max_arg = prob\n",
        "            solve = y\n",
        "    return solve\n",
        "\n",
        "def naive_ingenuo(instances, classes, percentage):\n",
        "  data_train, data_test, target_train, target_test = train_test_split(instances, classes, test_size=percentage)\n",
        "  trainingSize = len(data_train)\n",
        "  numFeatures = len(data_train[0])\n",
        "  testingSize = len(target_test)\n",
        "  good = 0\n",
        "\n",
        "  casesByFeature, data = training(data_train, target_train, trainingSize, numFeatures)\n",
        "  for i in range(testingSize):\n",
        "    predict = classify(numFeatures, trainingSize, casesByFeature, data, data_test[i])\n",
        "    if target_test[i] == predict:\n",
        "        good += 1\n",
        "  \n",
        "  dict_score = dict([('Instancias',testingSize), ('Bien clasificadas',good), ('Mal clasificados',testingSize-good), ('Precisión:',(good/testingSize)*100)])\n",
        "  return dict_score['Precisión:'] #dict_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9gZcv8cya3t"
      },
      "source": [
        "### Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP10D80LKSPu"
      },
      "source": [
        "<div style=\"text-align: justify\">Se desarrolló una función que toma como valores de entrada la base de datos dividida en atributos y clases, junto al tipo de discretización deseada. \n",
        "La función ejecuta los tres clasificadores con los datos ingresados $30$ veces y obtiene como salida la presición y desviación estándar calculadas. Para la discretización usando la librería de Sklearn se fijó el parámetro n_bins=10, sin embargo el valor se ajusta dependiendo de la base de datos ingresada. </div> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-cL4KwSBCkZy"
      },
      "outputs": [],
      "source": [
        "def scores(X,y, kind):\n",
        "    resultados={}\n",
        "    if kind==\"NP\":\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"CAIM\":\n",
        "        X=main_caim(X,y)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"kmeans\":\n",
        "        enc = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"kmeans\")\n",
        "        X = enc.fit_transform(X)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"quantile\":\n",
        "        enc = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n",
        "        X = enc.fit_transform(X)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}    \n",
        "    return pd.DataFrame(resultados).T.describe().iloc[1:3].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF1nNSaQ3JLH"
      },
      "source": [
        "## Pre-processing data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LDjvsopSdnkp"
      },
      "outputs": [],
      "source": [
        "xlist=[]\n",
        "ylist=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RRewcr73Bec"
      },
      "source": [
        "### Dataset 1 Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RaaAZv354zuC"
      },
      "outputs": [],
      "source": [
        "iris_database = load_iris()\n",
        "X = iris_database.data\n",
        "y = iris_database.target\n",
        "X_discret = main_caim(X,y)\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zztLcggw44sx"
      },
      "source": [
        "### Dataset 2 Skin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FB3NtwYM47I9"
      },
      "outputs": [],
      "source": [
        "ds = pd.read_csv('Skin_NonSkin.csv', sep='\\t', header=None)\n",
        "ds = ds.sample(frac=0.9)\n",
        "X = ds.iloc[:100,:-1]\n",
        "X = np.array(X)\n",
        "y = np.array(ds.iloc[:100,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXPyOPet496b"
      },
      "source": [
        "### Dataset 3 Cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qprpRQ1E4_P5"
      },
      "outputs": [],
      "source": [
        "cancer_dt = pd.read_csv('cancer.csv', header=None)\n",
        "cancer_dt.dropna(how='all')\n",
        "cancer_dt = cancer_dt.sample(frac=0.5)\n",
        "x_dt_cancer = cancer_dt.iloc[:,:-2]\n",
        "x_cancer = np.array(x_dt_cancer)\n",
        "\n",
        "for i in range(x_cancer.shape[0]):\n",
        "  for j in range(x_cancer.shape[1]):\n",
        "    if (x_cancer[i][j] == '?'):\n",
        "      x_cancer[i][j] = 0\n",
        "\n",
        "X = np.array(x_cancer, dtype=float)\n",
        "y = np.array(cancer_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjLfMKVO5ALm"
      },
      "source": [
        "### Dataset 4 Wine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wtuXhLj15B2b"
      },
      "outputs": [],
      "source": [
        "wine_dt = pd.read_csv('wine.csv', header=None)\n",
        "x_dt_wine = wine_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_wine)\n",
        "y = np.array(wine_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCZ7z2eQ5EVg"
      },
      "source": [
        "### Dataset 5 Vertebral\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O7GQNlI_5Gdt"
      },
      "outputs": [],
      "source": [
        "vertebral_dt = pd.read_csv('vertebral.csv', header=None)\n",
        "x_dt_vertebral = vertebral_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_vertebral)\n",
        "y = np.array(vertebral_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0aRKLzb5Lf5"
      },
      "source": [
        "### Dataset 6 Sonar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ShaP52kN5M-6"
      },
      "outputs": [],
      "source": [
        "sonar_dt = pd.read_csv('sonar.csv', header=None)\n",
        "x_dt_sonar = sonar_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_sonar)\n",
        "y = np.array(sonar_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmHQ8a1J5ROV"
      },
      "source": [
        "### Dataset 7 Occupancy detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "weiluYBX5Sru"
      },
      "outputs": [],
      "source": [
        "segmentation_dt = pd.read_csv('occupancy_detection.csv', header=None)\n",
        "segmentation_dt = segmentation_dt.sample(frac=0.1)\n",
        "x_dt_segmentation = segmentation_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_segmentation)\n",
        "y = np.array(segmentation_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnpw6BlbzwKg"
      },
      "source": [
        "### Dataset 8 Banknote authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fgaMcEwlzy9K"
      },
      "outputs": [],
      "source": [
        "segmentation_dt = pd.read_csv('data_banknote_authentication.csv', header=None)\n",
        "x_dt_segmentation = segmentation_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_segmentation)\n",
        "y = np.array(segmentation_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjltekno3zcW"
      },
      "source": [
        "### Dataset 9 Glass identification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "2vU1ZfV83-d0"
      },
      "outputs": [],
      "source": [
        "segmentation_dt = pd.read_csv('glass_identification.csv', header=None)\n",
        "x_dt_segmentation = segmentation_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_segmentation)\n",
        "y = np.array(segmentation_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWFLrgeZ37Ur"
      },
      "source": [
        "### Dataset 10 Abalon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KkvwY1kz33GW"
      },
      "outputs": [],
      "source": [
        "segmentation_dt = pd.read_csv('abalone.csv', header=None)\n",
        "segmentation_dt = segmentation_dt.sample(frac=0.2)\n",
        "x_dt_segmentation = segmentation_dt.iloc[:,:-1]\n",
        "X = np.array(x_dt_segmentation)\n",
        "y = np.array(segmentation_dt.iloc[:,-1])\n",
        "xlist.append(X)\n",
        "ylist.append(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSGKPX4DvVB5"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlTZt-PhNiFR"
      },
      "source": [
        "La función scores se aplicó iterativamente sobre cada base de datos para obtener el conjunto de métricas obtenidas a partir de todas las combinaciones posibles entre algoritmos de discretización y clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFEf9j2Y5hfs",
        "outputId": "1311dd3d-a929-4e70-8a54-34ecc57317c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
            "  \"decreasing the number of bins.\" % jj\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_discretization.py:233: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
            "  \"decreasing the number of bins.\" % jj\n"
          ]
        }
      ],
      "source": [
        "#Recorriendo todos los dataset\n",
        "#labels=[\"Dataset\" + str(x) for x in range(10)]\n",
        "labels = [\"Iris\", \"Skin\", \"Cancer\", \"Wine\", \"Vertebral\", \"Sonar\", \"Occupancy\", \"Banknote\", \"Glass\", \"Abalon\"]\n",
        "kinds = [\"NP\", \"CAIM\", \"kmeans\", \"quantile\"]\n",
        "allds=[]\n",
        "for i in range(10):\n",
        "    kindscores=[]\n",
        "    for element in kinds:\n",
        "        kindscores.append(scores(xlist[i],ylist[i],element))\n",
        "        df = pd.concat(kindscores, axis = 1, keys=kinds).T\n",
        "    allds.append(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BE6-ijQeTnq"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(allds, axis = 0, keys=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7-g2xEielJp"
      },
      "outputs": [],
      "source": [
        "df.round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Función auxiliar para plots"
      ],
      "metadata": {
        "id": "7cmU6GFYuH-i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWE59GhKgxgy"
      },
      "outputs": [],
      "source": [
        "def scores2(X,y, kind):\n",
        "    resultados={}\n",
        "    if kind==\"NP\":\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"CAIM\":\n",
        "        X=main_caim(X,y)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"kmeans\":\n",
        "        enc = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"kmeans\")\n",
        "        X = enc.fit_transform(X)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}\n",
        "    elif kind==\"quantile\":\n",
        "        enc = KBinsDiscretizer(n_bins=10, encode=\"ordinal\", strategy=\"quantile\")\n",
        "        X = enc.fit_transform(X)\n",
        "        for i in range(30):\n",
        "            r1=naive_ingenuo(X,y,0.3)\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "            r2=cross_val_score(clf, X_test, y_test, cv=2)[0]\n",
        "            clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
        "            r3=clf.score(X_test, y_test)\n",
        "            resultados[i]={\"NB\":r1,\"Arbol-Desicion\":r2*100,\"SVM\":r3*100}    \n",
        "    return pd.DataFrame(resultados)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizaciones"
      ],
      "metadata": {
        "id": "uTZYrPqupI4R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sin procesado de datos"
      ],
      "metadata": {
        "id": "pX45DxqkyuLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "rows = 2\n",
        "cols = 5\n",
        "axes=[]\n",
        "fig=plt.figure()\n",
        "for a in range(rows*cols):\n",
        "    #b=imgs[a]\n",
        "    axes.append( fig.add_subplot(rows, cols, a+1) )\n",
        "    if a==10:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "    else:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "\n",
        "    axes[-1].set_title(subplot_title)  \n",
        "    scores2(xlist[a],ylist[a],\"NP\").T.boxplot()\n",
        "img=fig.tight_layout()    \n",
        "plt.show(img)"
      ],
      "metadata": {
        "id": "ujRhhdr9uafR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicando CAIM"
      ],
      "metadata": {
        "id": "8TfSBOq3yxSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "rows = 2\n",
        "cols = 5\n",
        "axes=[]\n",
        "fig=plt.figure()\n",
        "for a in range(rows*cols):\n",
        "    #b=imgs[a]\n",
        "    axes.append( fig.add_subplot(rows, cols, a+1) )\n",
        "    if a==10:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "    else:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "\n",
        "    axes[-1].set_title(subplot_title)  \n",
        "    scores2(xlist[a],ylist[a],\"CAIM\").T.boxplot()\n",
        "img=fig.tight_layout()    \n",
        "plt.show(img)"
      ],
      "metadata": {
        "id": "Eta5jMJ2yd3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-medias"
      ],
      "metadata": {
        "id": "7J5fj_Pxy2UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "rows = 2\n",
        "cols = 5\n",
        "axes=[]\n",
        "fig=plt.figure()\n",
        "for a in range(rows*cols):\n",
        "    #b=imgs[a]\n",
        "    axes.append( fig.add_subplot(rows, cols, a+1) )\n",
        "    if a==10:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "    else:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "\n",
        "    axes[-1].set_title(subplot_title)  \n",
        "    scores2(xlist[a],ylist[a],\"kmeans\").T.boxplot()\n",
        "img=fig.tight_layout()    \n",
        "plt.show(img)"
      ],
      "metadata": {
        "id": "nfoThubOyf_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando cuantiles"
      ],
      "metadata": {
        "id": "zkLh5-apy7iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style('whitegrid')\n",
        "plt.rcParams[\"figure.figsize\"] = (20,8)\n",
        "rows = 2\n",
        "cols = 5\n",
        "axes=[]\n",
        "fig=plt.figure()\n",
        "for a in range(rows*cols):\n",
        "    #b=imgs[a]\n",
        "    axes.append( fig.add_subplot(rows, cols, a+1) )\n",
        "    if a==10:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "    else:\n",
        "        subplot_title=(\"Presición por modelo para el DS:\"+ labels[a])\n",
        "\n",
        "    axes[-1].set_title(subplot_title)  \n",
        "    scores2(xlist[a],ylist[a],\"quantile\").T.boxplot()\n",
        "img=fig.tight_layout()    \n",
        "plt.show(img)"
      ],
      "metadata": {
        "id": "7TQlymh-ymbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpdsCsA2gb5"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Kurgan, L. A., & Cios, K. J. (2004). CAIM discretization algorithm. IEEE transactions on Knowledge and Data Engineering, 16(2), 145-153.\n",
        "\n",
        "[2] [Scikit-learn: Machine Learning](https://scikit-learn.org/stable/index.html) in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
        "\n",
        "[3] Mitchell, T., Machine Learning. 1997.\n",
        "\n",
        "[4] https://archive.ics.uci.edu/ml/index.php"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xF1nNSaQ3JLH",
        "7cmU6GFYuH-i"
      ],
      "name": "Comparison between discretization methods",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}